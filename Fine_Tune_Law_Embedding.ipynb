{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kibeWzIR2jmD",
        "outputId": "83698f3c-81e5-4227-8367-4a67d96536bb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbM6RbZ22kLX",
        "outputId": "051d7903-3af8-4f95-bb72-2b6f433f9126"
      },
      "outputs": [],
      "source": [
        "%pip install sentence_transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd-tBk9Z2k4m",
        "outputId": "47ed7207-e3c8-406d-e849-73a531c5319e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
        "from sentence_transformers.evaluation import  SequentialEvaluator, InformationRetrievalEvaluator\n",
        "from sentence_transformers.util import cos_sim\n",
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "from sentence_transformers.training_args import BatchSamplers\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset, Features, Value\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnyK97Fe53B_"
      },
      "outputs": [],
      "source": [
        "corpus_features = Features({\n",
        "    'corpus_id' : Value(\"string\"),\n",
        "    'content' : Value(\"string\")\n",
        "})\n",
        "\n",
        "question_features = Features({\n",
        "    'question_id' : Value(\"string\"),\n",
        "    'question' : Value(\"string\")\n",
        "})\n",
        "\n",
        "qnc_features = Features({\n",
        "    'question_id' : Value(\"string\"),\n",
        "    'corpus_id' : Value(\"string\")\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/drive/MyDrive/Law_Assisstant/data_train/\"\n",
        "output_dir = \"/content/drive/MyDrive/Law_Assisstant/model\"\n",
        "basemodel_path = \"hiieu/halong_embedding\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w9JZ9xw2lgY"
      },
      "outputs": [],
      "source": [
        "corpus1 = load_dataset(\"csv\", data_files=dataset_dir + \"corpus.csv\", features = corpus_features)[\"train\"]\n",
        "queries1 = load_dataset(\"csv\", data_files=dataset_dir + \"questions.csv\", features = question_features)[\"train\"]\n",
        "relevant_docs_data1 = load_dataset(\"csv\", data_files=dataset_dir + \"qnc.csv\", features = qnc_features)[\"train\"]\n",
        "corpus2 = load_dataset(\"csv\", data_files=dataset_dir + \"corpus2.csv\", features = corpus_features)[\"train\"]\n",
        "queries2 = load_dataset(\"csv\", data_files=dataset_dir + \"questions2.csv\", features = question_features)[\"train\"]\n",
        "relevant_docs_data2 = load_dataset(\"csv\", data_files=dataset_dir + \"qnc2.csv\", features = qnc_features)[\"train\"]\n",
        "\n",
        "corpus = datasets.concatenate_datasets([corpus1,corpus2]).shuffle(seed = 7)\n",
        "queries = datasets.concatenate_datasets([queries1,queries2]).shuffle(seed = 7)\n",
        "relevant_docs_data = datasets.concatenate_datasets([relevant_docs_data1,relevant_docs_data2]).shuffle(seed = 7)\n",
        "\n",
        "# Convert the datasets to dictionaries\n",
        "corpus = dict(zip(corpus[\"corpus_id\"], corpus[\"content\"]))  # Our corpus (cid => document)\n",
        "queries = dict(zip(queries[\"question_id\"], queries[\"question\"]))  # Our queries (qid => question)\n",
        "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
        "for qid, corpus_ids in zip(relevant_docs_data[\"question_id\"], relevant_docs_data[\"corpus_id\"]):\n",
        "    qid = str(qid)\n",
        "    corpus_ids = str(corpus_ids)\n",
        "    if qid not in relevant_docs:\n",
        "        relevant_docs[qid] = set()\n",
        "    relevant_docs[qid].add(corpus_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGUq0-54wQCv",
        "outputId": "a494d64c-9710-44c5-800a-7855c5c1fd5c"
      },
      "outputs": [],
      "source": [
        "print(len(corpus))\n",
        "print(len(queries))\n",
        "print(len(relevant_docs_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydrSDT-244gn"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(basemodel_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Zi9umL_LsN"
      },
      "outputs": [],
      "source": [
        "matryoshka_dimensions = [768, 512, 256, 128] \n",
        "matryoshka_evaluators = []\n",
        "\n",
        "\n",
        "eval_set  = dict()\n",
        "n = len(relevant_docs_data)\n",
        "\n",
        "\n",
        "for i,(k,v) in enumerate(relevant_docs.items()):\n",
        "    if(i == 1000):\n",
        "        break\n",
        "    eval_set[k] = v\n",
        "\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(\n",
        "        queries=queries,\n",
        "        corpus=corpus,\n",
        "        relevant_docs=eval_set,\n",
        "        name=f\"dim_128\",\n",
        "        truncate_dim=128, \n",
        "        score_functions={\"cosine\": cos_sim},\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRc1MLAp7XgY",
        "outputId": "2fc5a5a0-443f-47e0-c8a8-1447b6c96437"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "print(len(eval_set))\n",
        "\n",
        "results = evaluator(model)\n",
        "for k,v in results.items():\n",
        "    print(k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtTWgLfp8GA7"
      },
      "outputs": [],
      "source": [
        "train_set  = dict()\n",
        "n = len(relevant_docs_data)\n",
        "\n",
        "\n",
        "for i,(k,v) in enumerate(relevant_docs.items()):\n",
        "    if(i >= 1000):\n",
        "      train_set[k] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfcXE58n8RRR",
        "outputId": "941bdd4c-e638-4959-de6e-3eb0249318ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_training_dataset(queries, corpus, relevant_docs):\n",
        "\n",
        "    dataset_dict = {\"anchors\": [], \"positives\": []}\n",
        "\n",
        "    for i, query_id, docs in enumerate(relevant_docs.items()):\n",
        "        for doc_id in docs:\n",
        "          dataset_dict[\"anchors\"].append(queries[query_id])\n",
        "          dataset_dict[\"positives\"].append(corpus[doc_id] )\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQXsYrMz8WKa",
        "outputId": "c054b41b-28c3-41c4-8924-d12ccb677979"
      },
      "outputs": [],
      "source": [
        "training_dataset = get_training_dataset(queries, corpus, relevant_docs)\n",
        "\n",
        "training_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm5I4HUc9RHO"
      },
      "outputs": [],
      "source": [
        "inner_loss = MultipleNegativesRankingLoss(model)\n",
        "\n",
        "loss = MatryoshkaLoss(model, inner_loss, matryoshka_dims=matryoshka_dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D95tSIWC9chl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    \n",
        "    num_train_epochs=5,                        \n",
        "    bf16=True,                                  \n",
        "    \n",
        "    per_device_train_batch_size=8,             \n",
        "    per_device_eval_batch_size=4,             \n",
        "    gradient_accumulation_steps=4,           \n",
        "    \n",
        "    warmup_ratio=0.1,                           \n",
        "    learning_rate=3e-5,                        \n",
        "    lr_scheduler_type=\"cosine\",                 \n",
        "    optim=\"adamw_torch_fused\",                 \n",
        "    batch_sampler=BatchSamplers.NO_DUPLICATES,  #No duplicate is good for Multi Negative Ranking Loss\n",
        "    \n",
        "    eval_strategy=\"steps\",                                      \n",
        "    metric_for_best_model=\"eval_dim_128_cosine_accuracy@3\",  # best score 128 dimension\n",
        "    load_best_model_at_end=True,              \n",
        "    \n",
        "    logging_steps= 215,                         \n",
        "    save_steps = 860,\n",
        "    save_total_limit=5,                      \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jx6N28c9qcP"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformerTrainer\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,  \n",
        "    train_dataset=training_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "LcTVYAi69szO",
        "outputId": "37041704-9747-4b0c-8782-ed86af20b91e"
      },
      "outputs": [],
      "source": [
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save the best model\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
